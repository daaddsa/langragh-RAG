# 4. 系统设计文档 (精简版)

## 1. 系统架构设计

### 1.1 总体架构
为适应“2周学生开发项目”的特点，本系统采用 **轻量级 B/S 架构**。前端使用 Streamlit 直接渲染 UI，后端使用 FastAPI 暴露核心逻辑，状态管理采用 In-Memory 方式。

```mermaid
graph LR
    User((用户)) -->|Browser| Frontend[Streamlit 前端]
    Frontend -->|HTTP Requests| Backend[FastAPI 后端]
    
    subgraph Backend Services
        Backend --> LangGraph[LangGraph 引擎]
        LangGraph -->|Read/Write| Memory[(MemorySaver)]
        LangGraph -->|API| Tavily[Tavily Search API]
        LangGraph -->|API| LLM[LLM API (OpenAI/Aliyun/DeepSeek)]
        Backend --> PDFEngine[PDF 生成器 (ReportLab)]
    end
```

### 1.2 模块划分
1.  **Frontend (Streamlit)**:
    *   `app.py`: 主入口，渲染聊天界面、侧边栏配置、调用后端接口。
2.  **Backend (FastAPI)**:
    *   `main.py`: 定义 `/chat`, `/pdf`, `/health` 等路由。
    *   `graph.py`: 定义 LangGraph 的 Node（Chatbot, ToolNode）和 Edge。
    *   `pdf_service.py`: 封装 ReportLab 逻辑，处理字体注册和 PDF 绘制。
    *   `schema.py`: 定义 Pydantic 数据模型。
    *   `state.py`: 定义 LangGraph 的状态结构。

## 2. 核心流程设计

### 2.1 智能搜索流程
这是系统的核心，利用 LangGraph 实现。

1.  **Start** -> **Chatbot Node**
    *   输入：用户最新的问题 + 历史上下文。
    *   动作：LLM 分析意图。如果需要搜索，返回 ToolCall；否则直接生成回复。
    *   输出：`AIMessage` (可能包含 `tool_calls`)。
2.  **Router**
    *   逻辑：检查 `AIMessage` 是否包含 `tool_calls`。
    *   分支：
        *   有 `tool_calls` -> 转至 **Tools Node**。
        *   无 `tool_calls` -> 转至 **End**。
3.  **Tools Node**
    *   动作：执行 Tavily Search API。
    *   输出：`ToolMessage` (包含搜索结果 JSON)。
    *   流向：-> **Chatbot Node** (LLM 再次根据搜索结果生成最终回答)。

### 2.2 PDF 生成流程
1.  用户在前端点击“生成 PDF”。
2.  前端将当前的会话历史 (`st.session_state.messages`) 打包。
3.  前端调用 `POST /pdf` 接口，发送 Session ID、标题和消息历史。
4.  后端 `pdf_service` 接收数据：
    *   注册中文字体 (`simsun.ttc`)。
    *   创建 A4 画布。
    *   绘制标题、生成时间。
    *   遍历消息历史，按角色 (User/Assistant) 绘制不同样式的文本。
5.  返回 PDF 二进制流。
6.  前端接收流并提供下载按钮。

## 3. 关键技术点设计

### 3.1 状态持久化 (Lite版)
使用 `langgraph.checkpoint.memory.MemorySaver`。
*   **特点**：数据存储在内存中，服务重启后丢失。
*   **理由**：简化部署，无需配置 SQLite/Postgres，适合演示和短期交互。

### 3.2 流式传输 (Streaming)
为避免长文本生成时的等待焦虑，后端 `/chat` 接口使用 Generator 实现流式响应。
*   **后端**：`graph.astream_events` 监听 `on_chat_model_stream` 事件，实时 Yield 字符。
*   **前端**：使用 `requests.post(..., stream=True)`，并用 `response.iter_content` 逐块读取解码，实现打字机效果。

### 3.3 中文 PDF 支持
ReportLab 默认不支持中文。解决方案：
*   在项目 `backend/assets/` 目录下放置 Windows 标准字体 `simsun.ttc`。
*   `pdf_service.py` 启动时动态加载该字体：`pdfmetrics.registerFont(TTFont(..., subfontIndex=0))`。
*   文件名处理：使用 `filename*=UTF-8''...` Header 格式，确保下载时中文文件名不乱码。

### 3.4 多模型兼容
后端通过 `langchain_openai.ChatOpenAI` 统一接口，但通过参数化配置 `base_url` 和 `model_name` 来支持不同厂商（如 DeepSeek, Aliyun Qwen, Moonshot）。

## 4. 部署方案
*   **依赖管理**：所有 Python 依赖列于 `requirements.txt`。
*   **启动方式**：
    *   后端：`uvicorn backend.app.main:app --host 0.0.0.0 --port 8000`
    *   前端：`streamlit run frontend/app.py`
*   **环境配置**：API Key 由用户在前端界面输入，运行时注入，无需修改配置文件。
